Reinforcement learning with verifiable rewards (RLVR) is the dominant paradigm for training LLM reasoning, yet it produces brittle models that exploit dataset patterns and generate unfaithful reasoning chains. We investigate whether collaborative debate between LLMs---where two models independently solve a problem, then critique each other's solutions before revising---can produce more accurate and robust mathematical reasoning. Using GPT-4.1, GPT-4.1-mini, and Claude Sonnet 4 on 80 GSM8K and 40 MATH-500 problems, we find that cross-model debate yields a 7.5\% absolute accuracy improvement on competition-level mathematics (77.5\% $\to$ 85.0\%) while providing only marginal gains on grade-school problems (+1.3\%). The improvement on hard problems is driven by genuine error correction: in 33\% of cases where both models independently failed, debate produced a correct answer---a capability that simple voting cannot achieve. However, debate does not improve robustness to problem rephrasings, and negative persuasion (correct agents adopting wrong answers) remains a risk, particularly in same-model configurations. Our results suggest that collaborative debate is most valuable at the boundary of model capability, where individual reasoning is unreliable and error patterns are heterogeneous. These findings provide empirical grounding for collaborative RLVR, a training paradigm where models learn to produce reasoning that survives peer scrutiny.
