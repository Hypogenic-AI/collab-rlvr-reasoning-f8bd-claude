\section{Conclusion}
\label{sec:conclusion}

We conducted a controlled empirical study of cross-model debate for mathematical reasoning, comparing it against single-agent, self-consistency, and same-model debate baselines. Our central finding is that cross-model debate improves accuracy by 7.5\% on competition-level mathematics (\mathfive), driven by genuine error correction that includes ``both wrong $\to$ correct'' transitions in 33\% of eligible cases. This capability is absent from voting-based methods and represents a qualitatively different improvement mechanism. On easier problems (\gsmk), debate provides minimal benefit because models already achieve high accuracy and make correlated errors.

These results have direct implications for the design of collaborative RLVR training systems. The debate mechanism works best at the boundary of model capability with architecturally diverse partners, suggesting that training should target hard problems and use cross-model debate rather than same-model configurations. The risk of negative persuasion motivates reward designs that penalize agents for abandoning correct reasoning under peer pressure. Debate does not address robustness to surface-level rephrasings, indicating that this failure mode requires complementary approaches.

Looking forward, we identify three priorities: (1) scaling the evaluation to achieve statistical significance on \mathfive, (2) extending to multi-round debate and truly heterogeneous model families (\eg GPT + Claude + Gemini), and (3) implementing collaborative RLVR training where models are optimized with GRPO to produce reasoning that survives peer scrutiny. The ``both wrong $\to$ correct'' phenomenon suggests that debate provides a qualitatively richer training signal than standard RLVR---one that could produce models with fundamentally more robust reasoning.
