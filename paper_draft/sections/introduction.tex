\section{Introduction}
\label{sec:intro}

Large language models trained with reinforcement learning from verifiable rewards (RLVR) have achieved remarkable performance on mathematical reasoning tasks \citep{guo2025deepseekr1, shao2024deepseekmath, yu2025dapo}. The recipe is simple: sample solutions, check answers against ground truth, and update the policy to favor correct outputs. This approach produced DeepSeek-R1's emergent chain-of-thought reasoning and DAPO's strong AIME performance---all without supervised fine-tuning on reasoning traces. Yet a growing body of evidence suggests that RLVR produces reasoning that is brittle and potentially unfaithful: models exploit dataset-specific patterns rather than learning generalizable strategies \citep{yue2025does}, and can achieve reward even with partially random training signals \citep{chen2025spurious}. If RLVR primarily selects pre-existing reasoning patterns rather than teaching new ones, how can we push models to develop genuinely robust reasoning?

We propose that the answer lies in \textit{collaboration}. When two models must solve a problem independently and then defend their reasoning to each other, they are forced to externalize their reasoning process, articulate their logic, and confront potential errors identified by their debate partner. This externalization mechanism goes beyond what single-agent RLVR can achieve: a model reasoning in isolation never has its intermediate steps challenged, whereas a model in debate must produce reasoning that survives scrutiny from a peer with different inductive biases.

\para{Our approach.} Before investing in expensive collaborative RLVR training, we validate the core premise empirically: \textit{does inference-time debate between LLMs improve mathematical reasoning?} We design controlled experiments comparing four conditions on identical problem sets: (1) single-agent chain-of-thought, (2) self-consistency via majority voting, (3) cross-model debate between architecturally different models, and (4) same-model debate between instances of the same model. We use GPT-4.1 as the primary agent, paired with GPT-4.1-mini and Claude Sonnet 4 for cross-model debate, and evaluate on 80 GSM8K and 40 MATH-500 problems.

\para{Key findings.} Cross-model debate produces a 7.5\% absolute accuracy improvement on competition-level MATH-500 problems (77.5\% $\to$ 85.0\%), driven by genuine error correction rather than simple agreement. In 33\% of cases where both models independently produced wrong answers, debate led to a correct solution---demonstrating emergent problem-solving that neither model achieves alone. On easier GSM8K problems, debate provides only marginal improvement (+1.3\%), confirming that its value concentrates at the boundary of model capability. Debate does not improve robustness to problem rephrasings, and negative persuasion (correct agents adopting wrong answers) occurs in 3.8\% of GSM8K cases.

We make the following contributions:
\begin{itemize}[leftmargin=*,itemsep=0pt,topsep=0pt]
    \item We conduct a controlled empirical study of multi-agent debate for mathematical reasoning, comparing cross-model debate, same-model debate, self-consistency, and single-agent baselines on identical problem sets across two difficulty levels.
    \item We demonstrate that cross-model debate enables genuine error correction on hard problems, including cases where both models independently fail but debate produces a correct answer---a capability absent from voting-based approaches.
    \item We identify the conditions under which debate helps versus hurts: it is most effective when models operate near their capability boundary with heterogeneous error patterns, and least effective on easy problems where models share systematic biases.
    \item We provide empirical grounding for collaborative RLVR by showing that the debate mechanism produces qualitatively different improvements from self-consistency, motivating training paradigms that reward reasoning robustness under peer scrutiny.
\end{itemize}
