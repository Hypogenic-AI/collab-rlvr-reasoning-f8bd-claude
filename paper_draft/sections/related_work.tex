\section{Related Work}
\label{sec:related}

\para{Reinforcement learning with verifiable rewards.}
RLVR has emerged as the dominant paradigm for training reasoning in LLMs. DeepSeek-R1 \citep{guo2025deepseekr1} demonstrated that pure RL with accuracy-based rewards can produce emergent chain-of-thought reasoning without supervised fine-tuning. The underlying algorithm, Group Relative Policy Optimization (GRPO) \citep{shao2024deepseekmath}, eliminates the need for a learned value function by normalizing advantages within sampled groups, making it significantly more memory-efficient than PPO. DAPO \citep{yu2025dapo} extended this with decoupled clip ratios and dynamic sampling for open-source RLVR at scale. However, recent work questions whether RLVR genuinely teaches new reasoning or merely selects pre-existing patterns \citep{yue2025does}, and shows that RLVR can work even with partially spurious rewards \citep{chen2025spurious}. These findings motivate our investigation of collaborative mechanisms that may push reasoning beyond what individual models already encode.

\para{Multi-agent debate for reasoning.}
\citet{du2023debate} established that multiple LLM agents debating can improve both factuality and reasoning beyond any individual agent. Their key finding---that debate enables genuine error correction even when all agents initially produce wrong answers---is central to our hypothesis. Subsequent work explored debate variations: MAD \citep{liang2023mad} introduced structured debate with a judge, Exchange-of-Thought \citep{yin2023exchange} proposed cross-model communication of intermediate reasoning steps, and ReConcile \citep{chen2023reconcile} used confidence-weighted round-table discussion. \citet{smit2023mad} benchmarked debate strategies across tasks, while \citet{xu2024rethinking} analyzed theoretical bounds of multi-agent approaches. Importantly, \citet{wu2025talk} identified failure modes including persuasion cascades and groupthink, cautioning that debate does not always help. Our work differs from these studies in three ways: (1) we use state-of-the-art models (GPT-4.1, Claude Sonnet 4) rather than earlier-generation models, (2) we conduct fine-grained error correction analysis to distinguish genuine debate benefits from agreement effects, and (3) we frame our study as empirical validation for collaborative RLVR training.

\para{Multi-agent RL for collaboration.}
MAPoRL \citep{li2025maporl} is the closest prior work to our broader research agenda. It uses multi-agent PPO to train models to collaborate through sequential turn-taking, with an influence-aware verification reward. Their finding that single-agent training is insufficient for learning collaboration---and that collaboration skills transfer across domains---supports our premise. However, MAPoRL differs from our proposal in fundamental ways: it uses PPO with a learned critic (vs.\ GRPO), learned verification rewards (vs.\ verifiable rewards), and sequential communication (vs.\ simultaneous debate). \citet{chen2024coevolving} explored cooperative multi-agent RL where agents co-evolve strategies, while \citet{zhang2023collaboration} applied social psychology frameworks to LLM collaboration. Our empirical study validates the premise that cross-model debate can improve reasoning---a necessary first step before investing in the more expensive collaborative RLVR training that these approaches suggest.

\para{Self-consistency and ensemble methods.}
Self-consistency \citep{wang2023selfconsistency} improves reasoning by sampling multiple chains of thought and taking a majority vote, providing a strong baseline for multi-sample approaches. Process reward models \citep{lightman2023verify} evaluate each reasoning step rather than just the final answer, offering finer-grained training signals. STaR \citep{zelikman2022star} bootstraps reasoning through self-generated rationales. A key distinction of debate over these approaches is that debate can produce correct answers even when all individual samples are wrong, because the critique process enables error identification and correction that simple voting cannot achieve. Our experiments explicitly test this distinction by comparing debate against self-consistency on identical problems.
